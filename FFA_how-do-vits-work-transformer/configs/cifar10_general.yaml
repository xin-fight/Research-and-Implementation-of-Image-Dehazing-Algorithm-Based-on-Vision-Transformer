---
# This configuration is for training with weak data augmentation,
# which was rarely used in the paper.
dataset:
  name: cifar10
  root: "../data"
  mean:
  - 0.4914
  - 0.4822
  - 0.4465
  std:
  - 0.2023
  - 0.1994
  - 0.201
  padding: 4
train:
  warmup_epochs: 1
  epochs: 200
  batch_size: 128
val:
  batch_size: 256
  n_ff: 1
model:
  stem: false
  block: {}
optim:
  name: SGD
  lr: 0.1
  momentum: 0.9
  nesterov: false
  weight_decay: 5.0e-04
  scheduler:
    name: MultiStepLR
    milestones:
    - 60
    - 120
    - 160
    gamma: 0.2
env: {}
