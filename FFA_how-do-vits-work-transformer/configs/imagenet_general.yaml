---
# This configuration is for training with weak data augmentation,
# which was rarely used in the paper.
dataset:
  name: imagenet
  root: "../data"
  mean:
  - 0.485
  - 0.456
  - 0.406
  std:
  - 0.229
  - 0.224
  - 0.225
train:
  warmup_epochs: 1
  epochs: 90
  batch_size: 256
val:
  batch_size: 256
  n_ff: 1
model:
  stem: true
  block: {}
optim:
  name: SGD
  lr: 0.1
  momentum: 0.9
  nesterov: false
  weight_decay: 1.0e-4
  scheduler:
    name: MultiStepLR
    milestones:
    - 30
    - 60
    - 80
    gamma: 0.1
env: {}
